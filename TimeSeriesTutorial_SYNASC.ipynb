{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086e55e5-3318-41ba-9d26-ffdd7cdce8a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the stock quote from July 2015 to December 2020\n",
    "# Pulled data from Yahoo Finance\n",
    "import math\n",
    "import pandas_datareader as web\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd8c4ad-81dd-4f38-8bd9-3270082389e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas_bokeh\n",
    "pandas_bokeh.output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17efb2b8-2be4-49c8-a27b-d6aeb6f21bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "from IPython.display import Video\n",
    "# Embed the video (replace 'your_video.mp4' with your actual video path)\n",
    "Video(\"video/pl_readme_gif_2_0.mp4\", embed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e44def-6828-4cdb-a22e-f1bfae1b3e70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Video(\"video/pt_dm_vid.mp4\", embed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d612b1ec-ce0a-49e5-91bc-ec27bf26ca3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "forex_path = \"FOREX/\"\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16de66b-2438-4193-8c44-9ee2c94aa1c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_period = \"2018-01-01\"\n",
    "end_period = \"2024-01-01\"\n",
    "fig_size = (1500,400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea967ad-da8f-4bd8-b495-b56012b98fb1",
   "metadata": {},
   "source": [
    "## Getting Yahoo Finance \n",
    "### GET THE DATE FOR MICROSOFT; APPLE; NVIDIA; INTEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b760a49-2bab-457d-8c02-88f6d6877e0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "Image(url= \"https://www.labsterx.com/wp-content/uploads/2017/03/yahoo-finance-summary.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c95857-9357-4281-9713-5fe7228c6136",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all = yf.download((\"MSFT\", \"AAPL\",\"NVDA\", \"INTC\", \"AMD\"), start=start_period, end=end_period)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e037871-0d9a-4356-9955-977bfb1e3085",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all.to_feather(\"data/yahoo.fa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5119a214-2499-4d51-8ab7-d67584bc0fa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(df_all.head().to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c75ae0f-6a94-4be6-8df1-36cd0d31b629",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all = df_all.Close\n",
    "display(HTML(df_all.head().to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33bd6c9-93a9-4459-8b6e-3577959b5dbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all.plot_bokeh(y = \"NVDA\", figsize = (1500,400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec67eab3-6a93-45de-b905-fe73b2a9553d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "# Plotting correlation heatmap\n",
    "dataplot = sb.heatmap(df_all.corr(numeric_only=True), cmap=\"YlGnBu\", annot=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7100b5-9748-4761-b8f9-1e005925dc7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_NVIDIA = pd.DataFrame(df_all[\"NVDA\"])\n",
    "display(HTML(df_NVIDIA.head().to_html()))\n",
    "df_NVIDIA.plot_bokeh(figsize = fig_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0d6f69-e377-4bf9-ae3d-b5f719149060",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Assuming 'data' is the time series data\n",
    "result = adfuller(df_NVIDIA)\n",
    "print('ADF Statistic:', result[0])\n",
    "print('p-value:', result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5865a3b-20ea-4e8e-84b8-68554c5e98e1",
   "metadata": {},
   "source": [
    "    ADF Statistic: The test statistic is compared to critical values at various significance levels (typically 1%, 5%, and 10%). If the ADF statistic is less than the critical value, then we reject the null hypothesis, implying the series is stationary.\n",
    "\n",
    "    Null Hypothesis (H₀): The series has a unit root (i.e., it is non-stationary).\n",
    "\n",
    "    Alternative Hypothesis (H₁): The series is stationary (i.e., no unit root).\n",
    "This strongly suggests that the time series is non-stationary, meaning it likely has a trend, seasonality, or some other structure that violates the assumption of stationarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9eb5ed-df53-4b8c-b12c-b78bb3e76595",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "fig = plot_acf(df_NVIDIA, lags=80)\n",
    "fig.set_figwidth(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c0b2e8-4965-46f7-a17d-9f7eac8d31f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.signal import detrend\n",
    "\n",
    "detrended = detrend(df_NVIDIA.iloc[:, 0], type='constant')\n",
    "detrended = pd.DataFrame(detrended, index=df_NVIDIA.index)\n",
    "pd.concat([df_NVIDIA,detrended]).plot_bokeh(figsize = fig_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8deb3e-c515-4dc0-b329-771d22648145",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "detrended = detrend(df_NVIDIA.iloc[:, 0], type='linear')\n",
    "detrended = pd.DataFrame(detrended, index=df_NVIDIA.index)\n",
    "detrended.columns= [\"DETREND\"]\n",
    "pd.concat([df_NVIDIA,detrended]).plot_bokeh(figsize = fig_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b6c8ac-c542-4a25-9825-98f5b0fc2bbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rolling_mean = df_NVIDIA.rolling(window=60, center=True).mean()\n",
    "detrended = pd.DataFrame(df_NVIDIA - rolling_mean)\n",
    "detrended.columns = ['DETREND MA']\n",
    "\n",
    "pd.concat([df_NVIDIA,detrended]).plot_bokeh(figsize = fig_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cc7af0-f5ea-4aec-b65a-5341aa37beed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_prepared = detrended.dropna().iloc[:,0]\n",
    "df_prepared\n",
    "Image(url= \"https://storage.googleapis.com/lightning-avatars/litpages/01hhda2ban5mpa8sa8gv0985cp/lstm_image.001.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424b9d64-ac03-40e8-9092-a8180c2b1bf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f6e272-19fb-4e7e-9034-6713b4447a51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom Dataset for LSTM with multi-step ahead prediction\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, seq_length, steps_ahead):\n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "        self.steps_ahead = steps_ahead\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length - self.steps_ahead + 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get the sequence of input data\n",
    "        x = self.data[index:index + self.seq_length]\n",
    "        # Get the target as the next 'steps_ahead' values\n",
    "        y = self.data[index + self.seq_length : index + self.seq_length + self.steps_ahead]\n",
    "        return torch.FloatTensor(x), torch.FloatTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2215fa-9a9d-4b03-9ae7-e987de8d7c74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PyTorch Lightning DataModule\n",
    "class StockDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, df, seq_length=5, steps_ahead=1, batch_size=32, split_ratio=0.8):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.seq_length = seq_length\n",
    "        self.steps_ahead = steps_ahead\n",
    "        self.batch_size = batch_size\n",
    "        self.split_ratio = split_ratio\n",
    "        self.scaler = MinMaxScaler()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Extract the stock prices and scale the data\n",
    "        data = self.df.values.reshape(-1, 1)\n",
    "        data = self.scaler.fit_transform(data)\n",
    "        data = data.flatten()\n",
    "\n",
    "        # Split data into training and test\n",
    "        train_size = int(len(data) * self.split_ratio)\n",
    "        self.train_data = data[:train_size]\n",
    "        self.val_data = data[train_size:]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = TimeSeriesDataset(self.train_data, self.seq_length, self.steps_ahead)\n",
    "        return DataLoader(train_dataset, batch_size=self.batch_size, num_workers=11, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataset = TimeSeriesDataset(self.val_data, self.seq_length, self.steps_ahead)\n",
    "        return DataLoader(val_dataset, batch_size=self.batch_size, num_workers=11 )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_dataset = TimeSeriesDataset(self.val_data, self.seq_length, self.steps_ahead)  # Using val data for test\n",
    "        return DataLoader(test_dataset, batch_size=self.batch_size, num_workers=11)\n",
    "    \n",
    "    def predict_dataloader(self):\n",
    "        return self.test_dataloader()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52df9ec5-0c01-4e9c-95b2-4de6e89b7523",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "# Define the LSTM model for multi-step ahead prediction\n",
    "class LSTMForecastingModel(pl.LightningModule):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, steps_ahead=1, lr=1e-3):\n",
    "        super(LSTMForecastingModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.steps_ahead = steps_ahead\n",
    "        self.lr = lr\n",
    "\n",
    "        # Define LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Define a fully connected layer to output the steps_ahead predictions\n",
    "        self.fc = nn.Linear(hidden_size, steps_ahead)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 2:  # Input shape could be [seq_length, input_size] without batch\n",
    "            x = x.unsqueeze(1)\n",
    "        # Initialize hidden and cell state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Pass the input through the LSTM layer\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        # Take the last output from the LSTM and pass it through the fully connected layer\n",
    "        out = self.fc(out[:,-1])\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log('test_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    # Function to predict for test dataset\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        x, _ = batch\n",
    "        return self(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a23443-62e3-48c4-87e6-2bd1791c2ee0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the DataModule with your desired number of steps ahead\n",
    "seq_length = 30\n",
    "steps_ahead = 5\n",
    "data_module = StockDataModule(df_prepared, seq_length=seq_length, steps_ahead=steps_ahead, batch_size=32)\n",
    "\n",
    "# Instantiate the LSTM model for 3-step ahead forecasting\n",
    "model = LSTMForecastingModel(input_size=seq_length, hidden_size=64, num_layers=2, steps_ahead=steps_ahead, lr=1e-3)\n",
    "\n",
    "# Define a PyTorch Lightning Trainer\n",
    "trainer = pl.Trainer(max_epochs=10)\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5078854b-20a8-49d1-8433-6115d75caaf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Validate the model\n",
    "trainer.validate(model, data_module)\n",
    "\n",
    "# Test the model\n",
    "trainer.test(model, data_module)\n",
    "\n",
    "# Predict on test data\n",
    "predictions = trainer.predict(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8248f510-13f9-4faa-abc4-00f1717b6264",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "def predict_model(model, trainer,data_module):\n",
    "    # Get the number of steps ahead the model is predicting\n",
    "    steps_ahead = model.steps_ahead\n",
    "\n",
    "    # Get the predictions for the test set (a list of tensors, each with shape [batch_size, steps_ahead])\n",
    "    predictions = trainer.predict(model, data_module)\n",
    "\n",
    "    # Concatenate the list of predictions into a single tensor (shape: [num_samples, steps_ahead])\n",
    "    predictions = torch.cat(predictions).cpu().detach().numpy()\n",
    "\n",
    "    # Get the actual target values from the test dataloader\n",
    "    actuals = []\n",
    "    test_dataloader = data_module.test_dataloader()\n",
    "    for batch in test_dataloader:\n",
    "        _, y = batch\n",
    "        actuals.append(y)\n",
    "\n",
    "    # Concatenate the actual values into a single tensor (shape: [num_samples, steps_ahead])\n",
    "    actuals = torch.cat(actuals).cpu().detach().numpy()\n",
    "\n",
    "    # Prepare an empty DataFrame to hold results with enough space for shifting\n",
    "    df_results = pd.DataFrame(index=range(len(predictions)))\n",
    "\n",
    "    # Populate the DataFrame with shifted predictions and actuals\n",
    "    for step in range(steps_ahead):\n",
    "        # Shift the predictions upwards, since they refer to future steps\n",
    "        df_results[f'Prediction_Step_{step + 1}'] = pd.Series(predictions[:, step]).shift(-step)\n",
    "        # Actuals do not need to be shifted\n",
    "        df_results[f'Actual_Step_{step + 1}'] = pd.Series(actuals[:, step])\n",
    "\n",
    "    # Create MultiIndex for columns (Step first, then Type)\n",
    "    columns = pd.MultiIndex.from_product(\n",
    "        [[f'Step_{i+1}' for i in range(steps_ahead)], ['Prediction', 'Actual']],\n",
    "        names=['Step', 'Type']\n",
    "    )\n",
    "\n",
    "    # Rearrange the DataFrame with MultiIndex columns\n",
    "    df_results.columns = columns\n",
    "\n",
    "    # Drop rows with NaN values caused by shifting\n",
    "    df_results.dropna(inplace=True)\n",
    "    return df_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ad74a1-5abc-449b-bea8-ae3fd9529951",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predict_model(model, trainer,data_module)[\"Step_5\"].plot_bokeh(figsize = fig_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9a348e-8f7a-474f-96ee-9ab8699deec3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Instantiate the DataModule with your desired number of steps ahead\n",
    "seq_length = 50\n",
    "steps_ahead = 10\n",
    "data_module = StockDataModule(df_NVIDIA, seq_length=seq_length, steps_ahead=steps_ahead, batch_size=32)\n",
    "\n",
    "# Instantiate the LSTM model for 3-step ahead forecasting\n",
    "model = LSTMForecastingModel(input_size=seq_length, hidden_size=64, num_layers=2, steps_ahead=steps_ahead, lr=1e-3)\n",
    "\n",
    "# Define a PyTorch Lightning Trainer\n",
    "trainer = pl.Trainer(max_epochs=100)\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Validate the model\n",
    "trainer.validate(model, data_module)\n",
    "\n",
    "# Test the model\n",
    "trainer.test(model, data_module)\n",
    "\n",
    "# Predict on test data\n",
    "predictions = trainer.predict(model, data_module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a01f916-9d5d-4f26-9793-16ba5d929325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predict_model(model, trainer,data_module)[\"Step_1\"].plot_bokeh(figsize = fig_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2f1076-bc7e-4cbf-a6ca-d68fca170bc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "forex_path = \"data/FOREX\"\n",
    "df = pd.read_csv(os.path.join(forex_path, \"2019.csv\"), header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21840281-12fe-4b9b-a247-2f20a0354cb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an empty list to store dataframes (each dataframe will be from a single CSV file)\n",
    "dataframes = []\n",
    "\n",
    "# Loop through all the files in the folder\n",
    "for filename in os.listdir(forex_path):\n",
    "    \n",
    "    # Check if the file ends with \".csv\" (so we only process CSV files)\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(forex_path, filename)  # Create the full path to the file\n",
    "\n",
    "        # Read the CSV file into a pandas dataframe\n",
    "        # 'parse_dates=[0]' tells pandas to interpret the first column as dates\n",
    "        # 'index_col=0' makes the first column the index of the dataframe (Datetime index)\n",
    "        df = pd.read_csv(file_path, parse_dates=[0], index_col=0, sep=\";\", header = None)\n",
    "        # We assume the second column has the values we need (this could vary based on your files)\n",
    "        # We only keep the second column (index 0 for the first, 1 for the second column)\n",
    "        df = df.iloc[:, [0]]  # This keeps only the second column (values)\n",
    "        # Drop duplicate indices (duplicate dates)\n",
    "        df = df[~df.index.duplicated(keep='first')]\n",
    "\n",
    "\n",
    "        # Append this dataframe to our list of dataframes\n",
    "        dataframes.append(df)\n",
    "\n",
    "# After we've gone through all the CSV files and created dataframes,\n",
    "# we concatenate (join) them all together into one dataframe along the 'Datetime' index\n",
    "# This means the dataframes will be aligned by their dates (the Datetime index)\n",
    "if dataframes:\n",
    "    joined_df = pd.concat(dataframes, axis=0)\n",
    "else:\n",
    "    print(\"No CSV files found in the folder.\")\n",
    "\n",
    "# Display the joined dataframe\n",
    "\n",
    "joined_df= joined_df.sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cecf164-6432-46a0-b985-bd407d1936be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "joined_df.columns = [\"USD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a56138-067a-4f09-af9a-1c60e0a41a9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming your current dataframe is named `full_df` and has a DatetimeIndex with minute-level data\n",
    "# Ensure 'full_df' has 'value' column with exchange rate data\n",
    "\n",
    "# Step 1: Define the complete date range from 2019-01-01 00:00 to 2023-12-31 23:59\n",
    "full_date_range = pd.date_range(start='2019-01-01 00:00', end='2023-12-31 23:59', freq='min')\n",
    "\n",
    "# Step 2: Reindex your dataframe to this full date range\n",
    "# This will introduce NaN values for any missing minutes\n",
    "full_df_reindexed = joined_df.reindex(full_date_range)\n",
    "\n",
    "\n",
    "# Step 3: Fill missing data at the beginning and end with forward-fill and backward-fill\n",
    "# First, use forward-fill for missing data at the beginning, then backward-fill for the end\n",
    "full_df_reindexed['USD'] = full_df_reindexed['USD'].ffill().bfill()\n",
    "\n",
    "# Step 3: Use spline interpolation (order=3 for cubic interpolation)\n",
    "# You can also experiment with other orders like 2 (quadratic) or higher\n",
    "full_df_reindexed['USD'] = full_df_reindexed['USD'].interpolate(method='spline', order=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb635b6-e28e-4a9e-8fe5-d46c872ee60c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "class MinuteToDayDataset(Dataset):\n",
    "    def __init__(self, df, scaler=None):\n",
    "        \"\"\"\n",
    "        Dataset for loading minute-level time series data grouped into daily chunks (1440 minutes per day).\n",
    "        \n",
    "        Args:\n",
    "        df (pandas.DataFrame): DataFrame with minute-level data and a 'value' column.\n",
    "        scaler (sklearn.preprocessing): Optional scaler for scaling the data.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.scaler = scaler\n",
    "\n",
    "        # Ensure that the dataframe index is a datetime index\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df.index):\n",
    "            raise ValueError(\"The dataframe index must be of datetime type.\")\n",
    "        self.df.columns = [\"USD\"]\n",
    "\n",
    "        # Apply scaling if a scaler is provided\n",
    "        self.daily_data = list(self.df.groupby(self.df.index.floor(\"d\")))\n",
    "        # Apply scaling if a scaler is provided\n",
    "        if self.scaler is not None:\n",
    "            self.scale_data()\n",
    "            \n",
    "    def scale_data(self):\n",
    "        \"\"\"\n",
    "        Scales the daily data using the provided scaler.\n",
    "        \"\"\"\n",
    "        for i in range(len(self.daily_data)):\n",
    "            self.daily_data[i][1]['USD'] = self.scaler.transform(self.daily_data[i][1]['USD'].values.reshape(-1, 1))\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        # The number of samples is equal to the number of full days in the data\n",
    "        return len(self.daily_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get the daily data at index `idx` and return the 'value' column as a numpy array\n",
    "        day_df = self.daily_data[idx][1]\n",
    "        return day_df['USD'].values.astype(np.float32),  idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcf41a0-3b43-4e84-827c-02456ba2b317",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 2: PyTorch Lightning DataModule to manage the dataset and DataLoaders\n",
    "class ExchangeRateDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, df, batch_size=32, train_val_test_split=(0.7, 0.15, 0.15)):\n",
    "        \"\"\"\n",
    "        DataModule to handle the loading and splitting of the dataset for train, validation, and test phases.\n",
    "        \n",
    "        Args:\n",
    "        df (pandas.DataFrame): DataFrame containing the exchange rate data.\n",
    "        batch_size (int): Batch size for DataLoaders.\n",
    "        train_val_test_split (tuple): Ratio for splitting the dataset into train, validation, and test sets.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "        self.train_val_test_split = train_val_test_split\n",
    "        self.scaler = MinMaxScaler()  # MinMaxScaler to scale between 0 and 1\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \"\"\"\n",
    "        Sets up the train, validation, and test datasets by splitting the original dataset.\n",
    "        This function is automatically called by PyTorch Lightning when appropriate.\n",
    "        \"\"\"\n",
    "        # Fit the scaler on the full dataset\n",
    "        self.scaler.fit(self.df['USD'].values.reshape(-1, 1))\n",
    "\n",
    "        # Create the dataset with daily groups and scaled data\n",
    "        full_dataset = MinuteToDayDataset(self.df, scaler=self.scaler)\n",
    "\n",
    "        # Calculate train, validation, and test split sizes\n",
    "        train_size = int(self.train_val_test_split[0] * len(full_dataset))\n",
    "        val_size = int(self.train_val_test_split[1] * len(full_dataset))\n",
    "        test_size = len(full_dataset) - train_size - val_size\n",
    "\n",
    "        # Perform the split\n",
    "        self.train_dataset, self.val_dataset, self.test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # Return the DataLoader for the training set\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # Return the DataLoader for the validation set\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        # Return the DataLoader for the test set\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7897760b-632f-4ce5-b9e5-e9b0ea5d0c07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "\n",
    "class ConvAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self, input_length=1440, latent_dim=24, learning_rate = 0.001):\n",
    "        super(ConvAutoEncoder, self).__init__()\n",
    "        # Save hyperparameters to log them\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.input_length = input_length\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Encoder: The layers that compress the data\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=7, stride=2, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=5, stride=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Latent space: Compress the sequence to a small, latent representation\n",
    "        self.fc1 = nn.Linear(64 * (input_length // 8), self.latent_dim)\n",
    "        self.fc2 = nn.Linear(self.latent_dim, 64 * (input_length // 8))\n",
    "        \n",
    "        # Decoder: The layers that reconstruct the data back to the original size\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_channels=64, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(in_channels=32, out_channels=16, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(in_channels=16, out_channels=1, kernel_size=7, stride=2, padding=3, output_padding=1),\n",
    "            nn.Sigmoid()  # Sigmoid to scale values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the encoder, latent space, and decoder\n",
    "        x = x.unsqueeze(1)  # Add a channel dimension for Conv1d\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten for the fully connected layers\n",
    "        \n",
    "        # Latent space\n",
    "        latent = self.fc1(x)\n",
    "        x = self.fc2(latent)\n",
    "        x = x.view(x.size(0), 64, -1)  # Reshape for the decoder\n",
    "        \n",
    "        # Decode the data\n",
    "        reconstruction = self.decoder(x)\n",
    "        return reconstruction.squeeze(1)  # Remove channel dimension\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Training step: Calculate reconstruction loss (MSE) for training\n",
    "        x, _ = batch\n",
    "        reconstruction = self.forward(x)\n",
    "        loss = nn.MSELoss()(reconstruction, x)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Validation step: Calculate reconstruction loss (MSE) for validation\n",
    "        x, _ = batch\n",
    "        reconstruction = self.forward(x)\n",
    "        loss = nn.MSELoss()(reconstruction, x)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Test step: Calculate reconstruction loss (MSE) and store real and predicted values\n",
    "        x, original_index = batch\n",
    "        reconstruction = self.forward(x)\n",
    "        loss = nn.MSELoss()(reconstruction, x)\n",
    "        self.log('test_loss', loss)\n",
    "        \n",
    "        # Store original and reconstructed values for later\n",
    "        self.real_values.append(x.cpu().detach().numpy())\n",
    "        self.reconstructed_values.append(reconstruction.cpu().detach().numpy())\n",
    "        self.real_indices.append(original_index)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Optimizer for the model\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "    \n",
    "    def on_test_epoch_start(self):\n",
    "        # Initialize lists to store original, reconstructed values, and their timestamps during testing\n",
    "        self.real_values = []\n",
    "        self.reconstructed_values = []\n",
    "        self.real_indices = []\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        # Predict step: Runs inference on the input batch to generate reconstructed values\n",
    "        x, original_index = batch\n",
    "        reconstruction = self.forward(x)\n",
    "        return reconstruction, x, original_index\n",
    "\n",
    "    def predict_dataloader(self, dataloader):\n",
    "        \"\"\"\n",
    "        This method performs predictions on the entire dataloader and returns the predictions and the real data in a DataFrame.\n",
    "        \"\"\"\n",
    "        # Store all the real and predicted values\n",
    "        all_reconstructed = []\n",
    "        all_real = []\n",
    "        all_indices = []\n",
    "\n",
    "        # Run predictions\n",
    "        self.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                reconstructed, real, index = self.predict_step(batch, None)\n",
    "                all_reconstructed.append(reconstructed.cpu().numpy())\n",
    "                all_real.append(real.cpu().numpy())\n",
    "                all_indices.append(index)\n",
    "\n",
    "        # Concatenate all batches\n",
    "        all_reconstructed = np.concatenate(all_reconstructed, axis=0)\n",
    "        all_real = np.concatenate(all_real, axis=0)\n",
    "        all_indices = np.concatenate(all_indices, axis=0)\n",
    "\n",
    "        # Create a DataFrame with original, reconstructed values, and their corresponding indices\n",
    "        results_df = pd.DataFrame({\n",
    "            'Date': all_indices.flatten(),\n",
    "            'Real Value': all_real.flatten(),\n",
    "            'Reconstructed Value': all_reconstructed.flatten()\n",
    "        })\n",
    "\n",
    "        return results_df\n",
    "    # New `predict` method with inverse scaling\n",
    "    def predict(self, datamodule, split='test'):\n",
    "        \"\"\"\n",
    "        This method runs inference using the DataModule's dataloaders and reconstructs the full day.\n",
    "        \n",
    "        Args:\n",
    "        datamodule: The PyTorch Lightning DataModule that contains dataloaders.\n",
    "        split: Which dataloader to use (train, val, or test).\n",
    "        \n",
    "        Returns:\n",
    "        A pandas DataFrame with the real values, reconstructed values, and full daily minute-level timestamps.\n",
    "        \"\"\"\n",
    "        # Select the appropriate dataloader based on the split ('train', 'val', or 'test')\n",
    "        if split == 'test':\n",
    "            dataloader = datamodule.test_dataloader()\n",
    "        elif split == 'val':\n",
    "            dataloader = datamodule.val_dataloader()\n",
    "        else:\n",
    "            dataloader = datamodule.train_dataloader()\n",
    "\n",
    "        # Store all the real and predicted values\n",
    "        all_reconstructed = []\n",
    "        all_real = []\n",
    "        all_full_day_index = []\n",
    "\n",
    "        # Run predictions\n",
    "        self.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():  # Disable gradient computation\n",
    "            for batch in dataloader:\n",
    "                x, original_index = batch\n",
    "                reconstruction = self.forward(x)  # Run the forward pass to get predictions\n",
    "\n",
    "                # Store original and reconstructed values\n",
    "                all_reconstructed.append(reconstruction.cpu().numpy())\n",
    "                all_real.append(x.cpu().numpy())\n",
    "\n",
    "\n",
    "        # Concatenate all batches of reconstructed and real values\n",
    "        all_reconstructed = np.concatenate(all_reconstructed, axis=0)\n",
    "        all_real = np.concatenate(all_real, axis=0)\n",
    "\n",
    "        # Inverse transform the reconstructed and real values to get them back to their original scale\n",
    "        scaler = datamodule.scaler\n",
    "        all_reconstructed = scaler.inverse_transform(all_reconstructed.reshape(-1, 1))\n",
    "        all_real = scaler.inverse_transform(all_real.reshape(-1, 1))\n",
    "\n",
    "        # Create a DataFrame with the original, reconstructed values, and their full day index\n",
    "        results_df = pd.DataFrame({\n",
    "            'Real Value': all_real.flatten(),\n",
    "            'Reconstructed Value': all_reconstructed.flatten()\n",
    "        })\n",
    "        return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a912025b-f467-4910-907e-389926dafe79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Load your data and initialize the DataModule\n",
    "# Assuming 'full_df' is your dataframe with minute-level exchange rate data\n",
    "data_module = ExchangeRateDataModule(df=full_df_reindexed, batch_size=32)\n",
    "\n",
    "# Step 5: Initialize the AutoEncoder model\n",
    "autoencoder = ConvAutoEncoder(input_length=1440, latent_dim = 24)\n",
    "\n",
    "# Step 6: Train the model using PyTorch Lightning Trainer\n",
    "trainer = pl.Trainer(max_epochs=10)\n",
    "\n",
    "# Step 7: Train and validate the model\n",
    "trainer.fit(autoencoder, data_module)\n",
    "\n",
    "# Step 8: Test the model\n",
    "trainer.test(autoencoder, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09086d3-473c-4ea1-a5d9-b6619905142f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_index = i * autoencoder.input_le\n",
    "results_df[0:1440]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf1a384-5eb9-4044-b0a6-6832e6b49ad5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run prediction and get the results in a DataFrame\n",
    "results_df = autoencoder.predict(data_module)\n",
    "results_df.plot_bokeh(figsize = fig_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973793a4-a245-4642-bf5a-4d716e242f1c",
   "metadata": {},
   "source": [
    "## MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32181615-f6d1-48cb-bf13-b16cdfdfd738",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install mlflow\n",
    "from pytorch_lightning.loggers import MLFlowLogger\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c72a31-005e-40dc-9318-a062825b1dd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlflow_logger = MLFlowLogger(\n",
    "    experiment_name='AutoencoderExperiment',\n",
    "    run_name='ConvAutoEncoderRun',\n",
    "    tracking_uri='file:./mlruns'  # Or use your actual MLflow tracking URI\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69479a1-27f9-4fa9-95a6-4b2b9cc75675",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Enable MLflow autologging for more detailed tracking (optional)\n",
    "mlflow.pytorch.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b486bfcd-b7b6-4b5d-83b0-7c1b911989ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 2: Load your data and initialize the DataModule\n",
    "data_module = ExchangeRateDataModule(df=full_df_reindexed, batch_size=32)\n",
    "\n",
    "# Step 3: Initialize the AutoEncoder model\n",
    "autoencoder = ConvAutoEncoder(input_length=1440, latent_dim=24)\n",
    "\n",
    "# Step 4: Train the model using PyTorch Lightning Trainer with MLflow logging\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20, logger=mlflow_logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd03a5ae-b569-4f78-a376-44fee3538b32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 5: Train and validate the model\n",
    "trainer.fit(autoencoder, data_module)\n",
    "\n",
    "# Step 6: Test the model\n",
    "trainer.test(autoencoder, datamodule=data_module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f608914d-9612-4cab-873d-13f077cf6c06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor, EarlyStopping, RichModelSummary, RichProgressBar\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "class ConvAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self, input_length=1440, latent_dim=24, learning_rate = 0.001):\n",
    "        super(ConvAutoEncoder, self).__init__()\n",
    "        # Save hyperparameters to log them\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.input_length = input_length\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Encoder: The layers that compress the data\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=7, stride=2, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=5, stride=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Latent space: Compress the sequence to a small, latent representation\n",
    "        self.fc1 = nn.Linear(64 * (input_length // 8), self.latent_dim)\n",
    "        self.fc2 = nn.Linear(self.latent_dim, 64 * (input_length // 8))\n",
    "        \n",
    "        # Decoder: The layers that reconstruct the data back to the original size\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_channels=64, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(in_channels=32, out_channels=16, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(in_channels=16, out_channels=1, kernel_size=7, stride=2, padding=3, output_padding=1),\n",
    "            nn.Sigmoid()  # Sigmoid to scale values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the encoder, latent space, and decoder\n",
    "        x = x.unsqueeze(1)  # Add a channel dimension for Conv1d\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten for the fully connected layers\n",
    "        \n",
    "        # Latent space\n",
    "        latent = self.fc1(x)\n",
    "        x = self.fc2(latent)\n",
    "        x = x.view(x.size(0), 64, -1)  # Reshape for the decoder\n",
    "        \n",
    "        # Decode the data\n",
    "        reconstruction = self.decoder(x)\n",
    "        return reconstruction.squeeze(1)  # Remove channel dimension\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Training step: Calculate reconstruction loss (MSE) for training\n",
    "        x, _ = batch\n",
    "        reconstruction = self.forward(x)\n",
    "        loss = nn.MSELoss()(reconstruction, x)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Validation step: Calculate reconstruction loss (MSE) for validation\n",
    "        x, _ = batch\n",
    "        reconstruction = self.forward(x)\n",
    "        loss = nn.MSELoss()(reconstruction, x)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Test step: Calculate reconstruction loss (MSE) and store real and predicted values\n",
    "        x, original_index = batch\n",
    "        reconstruction = self.forward(x)\n",
    "        loss = nn.MSELoss()(reconstruction, x)\n",
    "        self.log('test_loss', loss)\n",
    "        \n",
    "        # Store original and reconstructed values for later\n",
    "        self.real_values.append(x.cpu().detach().numpy())\n",
    "        self.reconstructed_values.append(reconstruction.cpu().detach().numpy())\n",
    "        self.real_indices.append(original_index)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        \n",
    "        # Scheduler to reduce learning rate on plateau\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, min_lr=1e-6)\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_loss',  # Reduce LR when val_loss plateaus\n",
    "                'interval': 'epoch',\n",
    "                'frequency': 1\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def on_test_epoch_start(self):\n",
    "        # Initialize lists to store original, reconstructed values, and their timestamps during testing\n",
    "        self.real_values = []\n",
    "        self.reconstructed_values = []\n",
    "        self.real_indices = []\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        # Predict step: Runs inference on the input batch to generate reconstructed values\n",
    "        x, original_index = batch\n",
    "        reconstruction = self.forward(x)\n",
    "        return reconstruction, x, original_index\n",
    "\n",
    "    def predict_dataloader(self, dataloader):\n",
    "        \"\"\"\n",
    "        This method performs predictions on the entire dataloader and returns the predictions and the real data in a DataFrame.\n",
    "        \"\"\"\n",
    "        # Store all the real and predicted values\n",
    "        all_reconstructed = []\n",
    "        all_real = []\n",
    "        all_indices = []\n",
    "\n",
    "        # Run predictions\n",
    "        self.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                reconstructed, real, index = self.predict_step(batch, None)\n",
    "                all_reconstructed.append(reconstructed.cpu().numpy())\n",
    "                all_real.append(real.cpu().numpy())\n",
    "                all_indices.append(index)\n",
    "\n",
    "        # Concatenate all batches\n",
    "        all_reconstructed = np.concatenate(all_reconstructed, axis=0)\n",
    "        all_real = np.concatenate(all_real, axis=0)\n",
    "        all_indices = np.concatenate(all_indices, axis=0)\n",
    "\n",
    "        # Create a DataFrame with original, reconstructed values, and their corresponding indices\n",
    "        results_df = pd.DataFrame({\n",
    "            'Date': all_indices.flatten(),\n",
    "            'Real Value': all_real.flatten(),\n",
    "            'Reconstructed Value': all_reconstructed.flatten()\n",
    "        })\n",
    "\n",
    "        return results_df\n",
    "    # New `predict` method with inverse scaling\n",
    "    def predict(self, datamodule, split='test'):\n",
    "        \"\"\"\n",
    "        This method runs inference using the DataModule's dataloaders and reconstructs the full day.\n",
    "        \n",
    "        Args:\n",
    "        datamodule: The PyTorch Lightning DataModule that contains dataloaders.\n",
    "        split: Which dataloader to use (train, val, or test).\n",
    "        \n",
    "        Returns:\n",
    "        A pandas DataFrame with the real values, reconstructed values, and full daily minute-level timestamps.\n",
    "        \"\"\"\n",
    "        # Select the appropriate dataloader based on the split ('train', 'val', or 'test')\n",
    "        if split == 'test':\n",
    "            dataloader = datamodule.test_dataloader()\n",
    "        elif split == 'val':\n",
    "            dataloader = datamodule.val_dataloader()\n",
    "        else:\n",
    "            dataloader = datamodule.train_dataloader()\n",
    "\n",
    "        # Store all the real and predicted values\n",
    "        all_reconstructed = []\n",
    "        all_real = []\n",
    "        all_full_day_index = []\n",
    "\n",
    "        # Run predictions\n",
    "        self.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():  # Disable gradient computation\n",
    "            for batch in dataloader:\n",
    "                x, original_index = batch\n",
    "                reconstruction = self.forward(x)  # Run the forward pass to get predictions\n",
    "\n",
    "                # Store original and reconstructed values\n",
    "                all_reconstructed.append(reconstruction.cpu().numpy())\n",
    "                all_real.append(x.cpu().numpy())\n",
    "\n",
    "\n",
    "        # Concatenate all batches of reconstructed and real values\n",
    "        all_reconstructed = np.concatenate(all_reconstructed, axis=0)\n",
    "        all_real = np.concatenate(all_real, axis=0)\n",
    "\n",
    "        # Inverse transform the reconstructed and real values to get them back to their original scale\n",
    "        scaler = datamodule.scaler\n",
    "        all_reconstructed = scaler.inverse_transform(all_reconstructed.reshape(-1, 1))\n",
    "        all_real = scaler.inverse_transform(all_real.reshape(-1, 1))\n",
    "\n",
    "        # Create a DataFrame with the original, reconstructed values, and their full day index\n",
    "        results_df = pd.DataFrame({\n",
    "            'Real Value': all_real.flatten(),\n",
    "            'Reconstructed Value': all_reconstructed.flatten()\n",
    "        })\n",
    "        return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e24f572-81a3-449d-9d26-c50ea99d7dbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Initialize MLflow logger\n",
    "mlflow_logger = MLFlowLogger(\n",
    "    experiment_name='AutoencoderExperiment',\n",
    "    run_name='ConvAutoEncoderRun',\n",
    "    tracking_uri='file:./mlruns'  # Adjust this based on your MLflow setup\n",
    ")\n",
    "\n",
    "# Step 2: Initialize the AutoEncoder model\n",
    "autoencoder = ConvAutoEncoder(input_length=1440, latent_dim=24, learning_rate=0.001)\n",
    "\n",
    "# Step 3: Define the ModelCheckpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Metric to monitor\n",
    "    dirpath='./checkpoints',  # Directory to save the model\n",
    "    filename='best-model-{epoch:02d}-{val_loss:.2f}',  # Naming pattern\n",
    "    save_top_k=1,  # Save only the best model\n",
    "    mode='min',  # Minimize the validation loss\n",
    ")\n",
    "\n",
    "# Step 4: Define the LearningRateMonitor callback\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "# Step 5: Load your data and initialize the DataModule\n",
    "data_module = ExchangeRateDataModule(df=full_df_reindexed, batch_size=32)\n",
    "# Step 6: Train the model using PyTorch Lightning Trainer with Eartly stopping\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitorea el val_loss\n",
    "    patience=5,  # Se detiene si no mejora después de 5 epochs\n",
    "    mode='min'  # Detiene si no mejora\n",
    ") \n",
    "rich_progress = RichProgressBar()\n",
    "rich_summary = RichModelSummary(max_depth=2)  # Max depth is adjustable for how deep the summary should go\n",
    "\n",
    "# Step 6: Train the model using PyTorch Lightning Trainer with MLflow logging\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,  # Increase epochs for better training\n",
    "    logger=mlflow_logger,  # Pass the MLflow logger to the Trainer\n",
    "    callbacks=[checkpoint_callback, lr_monitor, early_stop_callback, rich_progress, rich_summary],  # Add the callbacks for checkpoint and LR monitoring\n",
    "    log_every_n_steps=10  # Optional: log metrics every 10 steps\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Step 7: Train and validate the model\n",
    "trainer.fit(autoencoder, data_module)\n",
    "\n",
    "# Step 8: Test the model\n",
    "trainer.test(autoencoder, datamodule=data_module)\n",
    "\n",
    "# Step 9: Enable MLflow autologging for more detailed tracking (optional)\n",
    "mlflow.pytorch.autolog()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d314ff-3573-4ad8-88d2-8542b9ef6ebd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to train the model using Ray Tune\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCheckpointCallback\n",
    "def train_autoencoder(config, data_module):\n",
    "    # Step 1: Initialize MLflow logger\n",
    "    mlflow_logger = MLFlowLogger(\n",
    "        experiment_name='AutoencoderExperiment',\n",
    "        run_name='ConvAutoEncoderRun',\n",
    "        tracking_uri='file:./mlruns'  # Adjust this based on your MLflow setup\n",
    "    )\n",
    "\n",
    "    # Step 2: Initialize the AutoEncoder model\n",
    "    autoencoder = ConvAutoEncoder(input_length=config[\"input_length\"], latent_dim=config[\"latent_dim\"], learning_rate=config[\"learning_rate\"])\n",
    "\n",
    "    # Step 3: Define the ModelCheckpoint callback\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_loss',  # Metric to monitor\n",
    "        dirpath='./checkpoints',  # Directory to save the model\n",
    "        filename='best-model-{epoch:02d}-{val_loss:.2f}',  # Naming pattern\n",
    "        save_top_k=1,  # Save only the best model\n",
    "        mode='min',  # Minimize the validation loss\n",
    "    )\n",
    "\n",
    "    # Step 4: Define the LearningRateMonitor callback\n",
    "    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "    # Step 5: Load your data and initialize the DataModule\n",
    "    data_module = ExchangeRateDataModule(df=full_df_reindexed, batch_size=32)\n",
    "    # Step 6: Train the model using PyTorch Lightning Trainer with Eartly stopping\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor='val_loss',  # Monitorea el val_loss\n",
    "        patience=5,  # Se detiene si no mejora después de 5 epochs\n",
    "        mode='min'  # Detiene si no mejora\n",
    "    ) \n",
    "    rich_progress = RichProgressBar()\n",
    "    rich_summary = RichModelSummary(max_depth=2)  # Max depth is adjustable for how deep the summary should go\n",
    "\n",
    "    # Step 6: Train the model using PyTorch Lightning Trainer with MLflow logging\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=50,  # Increase epochs for better training\n",
    "        logger=mlflow_logger,  # Pass the MLflow logger to the Trainer\n",
    "        callbacks=[checkpoint_callback, lr_monitor, early_stop_callback, rich_progress, rich_summary],  # Add the callbacks for checkpoint and LR monitoring\n",
    "        log_every_n_steps=10  # Optional: log metrics every 10 steps\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Step 7: Train and validate the model\n",
    "    trainer.fit(autoencoder, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350f1f37-a49a-42e1-8aa4-0eac153597fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the hyperparameter search space\n",
    "search_space = {\n",
    "    \"learning_rate\": tune.loguniform(1e-5, 1e-3),\n",
    "    \"latent_dim\": tune.choice([16, 24, 32]),\n",
    "    \"input_length\": 1440  # Fixed in this case, but can be a tunable parameter if needed\n",
    "}\n",
    "\n",
    "# Initialize Ray Tune search\n",
    "analysis = tune.run(\n",
    "    tune.with_parameters(train_autoencoder, data_module=data_module),  # Pass the function and data module\n",
    "    config=search_space,\n",
    "    num_samples=10,  # Number of samples to run\n",
    "    resources_per_trial={\"cpu\": 2, \"gpu\": 0},  # Adjust based on your system's resources\n",
    "    metric=\"val_loss\",  # Metric to optimize\n",
    "    mode=\"min\"  # We want to minimize validation loss\n",
    ")\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best hyperparameters found were: \", analysis.best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5308342d-a740-4884-bcbc-73ac8500b4df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# Model that can be LSTM, GRU, or RNN depending on the network_type hyperparameter\n",
    "class RNNForecastingModel(pl.LightningModule):\n",
    "    def __init__(self, network_type='LSTM', input_size=1, hidden_size=64, num_layers=2, steps_ahead=1, lr=1e-3):\n",
    "        super(RNNForecastingModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.steps_ahead = steps_ahead\n",
    "        self.lr = lr\n",
    "        self.network_type = network_type\n",
    "\n",
    "        # Initialize RNN, GRU, or LSTM based on network_type\n",
    "        if self.network_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        elif self.network_type == 'GRU':\n",
    "            self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        elif self.network_type == 'RNN':\n",
    "            self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid network_type. Choose from 'LSTM', 'GRU', or 'RNN'.\")\n",
    "\n",
    "        # Fully connected layer for output\n",
    "        self.fc = nn.Linear(hidden_size, steps_ahead)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 2:  # If input shape is [seq_length, input_size] without batch dimension\n",
    "            x = x.unsqueeze(1)\n",
    "        \n",
    "        # Initialize hidden state (and cell state if LSTM)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        if self.network_type == 'LSTM':\n",
    "            c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "            out, _ = self.rnn(x, (h0, c0))  # LSTM requires both hidden and cell state\n",
    "        else:\n",
    "            out, _ = self.rnn(x, h0)  # GRU and RNN only require hidden state\n",
    "\n",
    "        # Pass the last output of the RNN/GRU/LSTM through the fully connected layer\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "# Function to run training with Ray Tune\n",
    "def train_tune(config, data_module=None):\n",
    "    model = RNNForecastingModel(\n",
    "        network_type=config[\"network_type\"],  # LSTM, GRU, or RNN\n",
    "        input_size=config[\"input_size\"],\n",
    "        hidden_size=config[\"hidden_size\"],\n",
    "        num_layers=config[\"num_layers\"],\n",
    "        steps_ahead=config[\"steps_ahead\"],\n",
    "        lr=config[\"lr\"]\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=10,\n",
    "        gpus=1 if torch.cuda.is_available() else 0,\n",
    "        logger=TensorBoardLogger(save_dir=tune.get_trial_dir(), name=\"\", version=\"\"),\n",
    "        callbacks=[\n",
    "            TuneReportCallback({\"val_loss\": \"val_loss\"}, on=\"validation_end\")\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    trainer.fit(model, data_module)\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "config = {\n",
    "    \"network_type\": tune.choice([\"LSTM\", \"GRU\", \"RNN\"]),  # Choose model type\n",
    "    \"input_size\": tune.choice([30, 50]),  # Example input sequence lengths\n",
    "    \"hidden_size\": tune.choice([64, 128]),  # Hidden layer size\n",
    "    \"num_layers\": tune.choice([1, 2, 3]),  # Number of RNN layers\n",
    "    \"steps_ahead\": 1,  # One step ahead forecasting (fixed in this case)\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-2)  # Learning rate search space\n",
    "}\n",
    "\n",
    "# Setup Ray Tune's scheduler and reporter\n",
    "scheduler = ASHAScheduler(\n",
    "    max_t=10,\n",
    "    grace_period=1,\n",
    "    reduction_factor=2\n",
    ")\n",
    "\n",
    "reporter = CLIReporter(\n",
    "    parameter_columns=[\"network_type\", \"input_size\", \"hidden_size\", \"num_layers\", \"lr\"],\n",
    "    metric_columns=[\"val_loss\", \"training_iteration\"]\n",
    ")\n",
    "\n",
    "# Launch the Ray Tune hyperparameter search\n",
    "tune.run(\n",
    "    tune.with_parameters(train_tune, data_module=data_module),\n",
    "    resources_per_trial={\"cpu\": 2, \"gpu\": 1 if torch.cuda.is_available() else 0},\n",
    "    metric=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    config=config,\n",
    "    num_samples=10,  # Number of hyperparameter configurations to try\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=reporter\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0b76f4-67b9-448c-975d-07852f8f76b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "https://www.kaggle.com/datasets/utathya/future-volume-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a880321-bca2-46ea-a710-afdff5e61466",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pytorch_forecasting.data.examples import get_stallion_data\n",
    "\n",
    "df_stallion = get_stallion_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51644be9-d667-4357-9b1e-58b0b1ca9f0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_stallion.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf881dcb-bbf0-41f0-b8af-ad788324d83d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# add time index\n",
    "df_stallion[\"time_idx\"] = df_stallion[\"date\"].dt.year * 12 + df_stallion[\"date\"].dt.month\n",
    "df_stallion[\"time_idx\"] -= df_stallion[\"time_idx\"].min()\n",
    "\n",
    "# add additional features\n",
    "df_stallion[\"month\"] = df_stallion.date.dt.month.astype(str).astype(\"category\")  # categories have be strings\n",
    "df_stallion[\"log_volume\"] = np.log(df_stallion.volume + 1e-8)\n",
    "df_stallion[\"avg_volume_by_sku\"] = df_stallion.groupby([\"time_idx\", \"sku\"], observed=True).volume.transform(\"mean\")\n",
    "df_stallion[\"avg_volume_by_agency\"] = df_stallion.groupby([\"time_idx\", \"agency\"], observed=True).volume.transform(\"mean\")\n",
    "\n",
    "# we want to encode special days as one variable and thus need to first reverse one-hot encoding\n",
    "special_days = [\n",
    "    \"easter_day\",\n",
    "    \"good_friday\",\n",
    "    \"new_year\",\n",
    "    \"christmas\",\n",
    "    \"labor_day\",\n",
    "    \"independence_day\",\n",
    "    \"revolution_day_memorial\",\n",
    "    \"regional_games\",\n",
    "    \"fifa_u_17_world_cup\",\n",
    "    \"football_gold_cup\",\n",
    "    \"beer_capital\",\n",
    "    \"music_fest\",\n",
    "]\n",
    "df_stallion[special_days] = df_stallion[special_days].apply(lambda x: x.map({0: \"-\", 1: x.name})).astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3a3423-d938-46ce-a557-39d9d068d22c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_stallion.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7016c19-69b4-48de-be3a-ae75de161a01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_stallion[\"agency_number\"] = df_stallion[\"agency\"].str.extract('(\\d+)')\n",
    "\n",
    "# If you want to convert the extracted numbers to integer\n",
    "df_stallion['agency_number'] = pd.to_numeric(df_stallion['agency_number'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeff8ac8-aa1d-4196-a482-699c29be1554",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_stallion[\"sku_number\"] = df_stallion[\"sku\"].str.extract('(\\d+)')\n",
    "\n",
    "# If you want to convert the extracted numbers to integer\n",
    "df_stallion['sku_number'] = pd.to_numeric(df_stallion['sku_number'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548b6948-b7f1-4459-8838-a971b3d474cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_stallion_reduced = df_stallion[['volume', 'date', 'industry_volume', 'soda_volume',\n",
    "       'avg_max_temp', 'price_regular', 'price_actual', 'discount',\n",
    "       'avg_population_2017', 'avg_yearly_household_income_2017', \n",
    "       'discount_in_percent', 'timeseries', 'time_idx', 'month', 'log_volume',\n",
    "       'avg_volume_by_sku', 'avg_volume_by_agency', 'agency_number', \"sku_number\"]]\n",
    "df_stallion_reduced.loc[:,\"date\"] = pd.to_datetime(df_stallion_reduced[\"date\"])\n",
    "# Ensure nanosecond precision by casting to 'datetime64[ns]'\n",
    "df_stallion_reduced.loc[:,\"date\"]  = df_stallion_reduced['date'].astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ad5095-f482-466e-b1f5-45a6065944ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(HTML(df_stallion_reduced.head().to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e234d06-1e36-465f-bdcb-8414274795ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_prediction_length = 6\n",
    "max_encoder_length = 24\n",
    "import warnings\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "warnings.filterwarnings(\"ignore\", message=\"Converting non-nanosecond precision datetime values to nanosecond precision\")\n",
    "\n",
    "from darts import TimeSeries\n",
    "target_series = TimeSeries.from_group_dataframe(df_stallion_reduced,time_col=\"date\", value_cols='volume', group_cols=['agency_number', \"sku_number\"])\n",
    "\n",
    "# Similarly, create a TimeSeries for covariates, if needed\n",
    "covariates_series = TimeSeries.from_group_dataframe(\n",
    "    df_stallion_reduced, \n",
    "    time_col='date', \n",
    "    value_cols=['industry_volume', 'soda_volume', 'avg_max_temp', 'price_regular', \n",
    "                'price_actual', 'discount', 'avg_population_2017', \n",
    "                'avg_yearly_household_income_2017', 'discount_in_percent', \n",
    "                'month', 'log_volume'],\n",
    "    group_cols=['agency_number', 'sku_number']\n",
    ")\n",
    "\n",
    "scaler = Scaler()\n",
    "target_series_scaled = scaler.fit_transform(target_series)\n",
    "covariates_series_scaled = scaler.fit_transform(covariates_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acd7456-8eab-4d70-aa88-6701bac712c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from darts.utils.model_selection import train_test_split\n",
    "target_series_scaled_train, target_series_scaled_test = train_test_split(target_series_scaled, 0.10)\n",
    "covariates_series_scaled_train, covariates_series_scaled_test = train_test_split(covariates_series_scaled, 0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266700ef-73e3-456c-b14a-81962a735ea0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from darts.models import TransformerModel\n",
    "from darts.utils.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Define the Transformer model with past covariates\n",
    "transformer_model = TransformerModel(\n",
    "    input_chunk_length=30,       # How many past time steps the model looks at\n",
    "    output_chunk_length=1,       # How many future steps to predict (e.g., next month)\n",
    "    d_model=64,                  # Dimension of the model\n",
    "    nhead=4,                     # Number of attention heads\n",
    "    num_encoder_layers=2,        # Number of encoder layers\n",
    "    num_decoder_layers=2,        # Number of decoder layers\n",
    "    dropout=0.1,                 # Dropout to prevent overfitting\n",
    "    batch_size=32,               # Batch size\n",
    "    n_epochs=10,                # Number of epochs for training\n",
    "    random_state=42              # For reproducibility\n",
    ")\n",
    "\n",
    "\n",
    "# Fit the model with past covariates\n",
    "transformer_model.fit(\n",
    "    series=target_series_scaled_train, \n",
    "    past_covariates=covariates_series_scaled_train,  # Pass the past covariates here\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e634d85-5acb-4f96-a3dd-050f1dfc6963",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "backtest = transformer_model.historical_forecasts(\n",
    "    series=target_series_scaled_test[0],\n",
    "    past_covariates=covariates_series_scaled_test[0],\n",
    "    start=covariates_series_scaled_test[0].start_time(),\n",
    "    forecast_horizon=1,\n",
    "    retrain=False,\n",
    "    verbose=True,\n",
    ")\n",
    "target_series_scaled_test[0].plot(label=\"actual\")\n",
    "backtest.plot(label=\"backtest (H=1)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaab9bd-2bd3-49c7-a2e1-6d3c85ec5f65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca3a292-39eb-4f32-997a-fe022c1a7269",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from darts.models import RNNModel\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "\n",
    "# Optionally scale the data\n",
    "scaler = Scaler()\n",
    "target_series_scaled = scaler.fit_transform(target_series)\n",
    "\n",
    "# Define future covariates if you have them, e.g., cyclic encoders for time features like month\n",
    "add_encoders = {\n",
    "    'cyclic': {'future': ['month']}  # Adding future covariates automatically\n",
    "}\n",
    "\n",
    "# Define the LSTM model\n",
    "lstm_model = RNNModel(\n",
    "    model='LSTM',                  # Specify LSTM as the model type\n",
    "    input_chunk_length=12,         # How much history to look at\n",
    "    n_rnn_layers=1,                # Number of RNN layers\n",
    "    dropout=0.1,                   # Dropout rate\n",
    "    batch_size=32,                 # Batch size for training\n",
    "    n_epochs=20,                  # Number of training epochs\n",
    "    training_length=50,            # How much history is used during training\n",
    "    add_encoders=add_encoders      # Automatically generate future covariates\n",
    ")\n",
    "\n",
    "# Fit the LSTM model (without past covariates, but with future encoders)\n",
    "lstm_model.fit(series=target_series_scaled, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473155da-d7a6-45c4-84b6-bac154f47662",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "backtest = lstm_model.historical_forecasts(\n",
    "    series=target_series_scaled_test[0],\n",
    "    past_covariates=covariates_series_scaled_test[0],\n",
    "    start=covariates_series_scaled_test[0].start_time(),\n",
    "    forecast_horizon=1,\n",
    "    retrain=False,\n",
    "    verbose=True,\n",
    ")\n",
    "target_series_scaled_test[0].plot(label=\"actual\")\n",
    "backtest.plot(label=\"backtest (H=1)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087979dd-f12a-403d-9cee-65e5d9b7b3fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pytrends.request import TrendReq\n",
    "import pandas as pd\n",
    "\n",
    "pytrend = TrendReq()\n",
    "\n",
    "# Define the keyword you want to search for\n",
    "keywords = [\"CRYPTO\",\"AI\"]\n",
    "\n",
    "# Fetch interest over time\n",
    "pytrends.build_payload(keywords, cat=0, timeframe=f\"{start_period} {end_period}\", geo='US', gprop='')\n",
    "df = pytrends.interest_over_time()\n",
    "\n",
    "df.plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d934944-2997-4f61-8991-8bac3e88e8fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.plot_bokeh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57ead43-c0f7-413b-86d1-59124e24243c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collect_trend_score(keyword):\n",
    "    pytrend = TrendReq() \n",
    "    pytrend.build_payload(kw_list=[keyword], timeframe='2020-01-01 2022-01-01')\n",
    "    df = pytrend.interest_over_time()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bcb70d-d9f3-44ed-95d4-293d3293c735",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def resample_trend_score_df(df, keyword):\n",
    "    trends = df[keyword].resample('D', convention = 'start').pad()\n",
    "    trends = pd.DataFrame(trends)\n",
    "    trends.rename(columns = {keyword:'trend_score'}, inplace = True)\n",
    "    return trends"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:BMS]",
   "language": "python",
   "name": "conda-env-BMS-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
